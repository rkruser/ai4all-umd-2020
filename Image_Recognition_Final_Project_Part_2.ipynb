{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Recognition Final Project Part 2",
      "provenance": [],
      "collapsed_sections": [
        "Lyvdo_Wz3sre"
      ],
      "toc_visible": true,
      "mount_file_id": "1EQ-jPpUpGVwf_Kqq0dk8tFAXsaTVySkJ",
      "authorship_tag": "ABX9TyOZnn9/TCEx40nL/u7RDmis",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkruser/ai4all-umd-2020/blob/master/Image_Recognition_Final_Project_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuV4uigx3Vhv",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sftopikfeZqp",
        "colab_type": "text"
      },
      "source": [
        "## Setting up the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsFPcoyd3Xs4",
        "colab_type": "text"
      },
      "source": [
        "In this colab, you will put together everything you have learned. First, let's install necessary software and download the github repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJb88dZRq3hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install flask-ngrok\n",
        "!git clone https://github.com/rkruser/ai4all-umd-2020.git #https seems to work, but not ssh\n",
        "%cd ai4all-umd-2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I51xIbT23kuc",
        "colab_type": "text"
      },
      "source": [
        "Then download the leafsnap dataset and unpack it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUPPEtZ2q9um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data\n",
        "!mkdir data/leafsnap\n",
        "!curl -o ./data/leafsnap/leafsnap.tar http://leafsnap.com/static/dataset/leafsnap-dataset.tar\n",
        "!tar -xvf ./data/leafsnap/leafsnap.tar -C ./data/leafsnap > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AJBVgliAK-9",
        "colab_type": "text"
      },
      "source": [
        "**Finally, follow these drive links** to download \n",
        "[resnet trained on Leafsnap](https://drive.google.com/file/d/156hbvRB-EkQTkyMIjru6lye-5TUAosjF/view?usp=sharing) and [your neural network](https://drive.google.com/file/d/1YH70L-pESc8m4N-vgPjVk6YbnUIEUo6k/view?usp=sharing) trained on leafsnap. Save these files on your computer, then manually upload them to `ai4all-umd-2020/models/resnet` and `ai4all-umd-2020/models/your_model` respectively by using the folder interface on the left side of the Colab. (Expand the desired folder, then click on the three dots to the right of its name, and then click \"upload\").\n",
        "\n",
        "**Uploading may take a minute. The bottom left corner of Colab should show upload progress.** You may get code errors if you try to run cells before the upload is complete.\n",
        "\n",
        "**Note that you will have to re-upload these if Colab decides to obliterate your file system.** Manual uploads are the simplest solution for right now. The colab will not be wiped clean too often if you are active on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyvdo_Wz3sre",
        "colab_type": "text"
      },
      "source": [
        "## Configuring Git\n",
        "**Note** that if you're just running the colab without making updates to the github project, you can skip the configuring / using git steps.\n",
        "\n",
        "\n",
        "Here we configure git so that you can push and pull to the repository from your colab. Replace {Your github email}, {Your github username}, and {your github password} with the appropriate information (delete the brackets too), then run the following code cell. When you're done, hide this cell, as it contains your github password."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUxsBj7ksJAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git config --global user.email \"{your git email}\"\n",
        "!git config --global user.name \"{your git username}\"\n",
        "!git remote set-url origin \"https://{your git username}:{your git password}@github.com/rkruser/ai4all-umd-2020.git\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JNEnST94mFe",
        "colab_type": "text"
      },
      "source": [
        "## Using Git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJIdIp94o5S",
        "colab_type": "text"
      },
      "source": [
        "\"git status -s\" checks whether there are changes that need to be committed\n",
        "\n",
        "Run \"git add\" and \"git commit\" to save the changes to git when needed\n",
        "\n",
        "Run \"git push origin master\" to push changes to everyone.\n",
        "\n",
        "Run \"git pull origin master\" to pull changes made by other people. If there is a merge conflict, you may need to resolve it, then run a \"git add\" / \"git commit\" to save the changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL7o2A_OtwaV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "8d72a639-5dd8-47c2-cdbe-659c888bbc3d"
      },
      "source": [
        "!git status\n",
        "!git add app_functions.py\n",
        "!git commit -m 'Added app_functions'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mapp_functions.py\u001b[m\n",
            "\t\u001b[31mmodels/resnet/resnet_model_49.pth\u001b[m\n",
            "\t\u001b[31mmodels/your_model/your_model_49.pth\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "[master 3b22024] Added app_functions\n",
            " 1 file changed, 175 insertions(+)\n",
            " create mode 100644 app_functions.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhFYk_j8wcJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e6ef3f38-c2a5-479f-d8e0-1fa220db607d"
      },
      "source": [
        "!git push origin master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 3, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  33% (1/3)   \rCompressing objects:  66% (2/3)   \rCompressing objects: 100% (3/3)   \rCompressing objects: 100% (3/3), done.\n",
            "Writing objects:  33% (1/3)   \rWriting objects:  66% (2/3)   \rWriting objects: 100% (3/3)   \rWriting objects: 100% (3/3), 2.21 KiB | 2.21 MiB/s, done.\n",
            "Total 3 (delta 1), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/rkruser/ai4all-umd-2020.git\n",
            "   49b37be..3b22024  master -> master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXuFVpK0xU6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "14913ca8-f2d4-4c3d-9bfe-edd6e2de3ca7"
      },
      "source": [
        "!git pull origin master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:   8% (1/12)\u001b[K\rremote: Compressing objects:  16% (2/12)\u001b[K\rremote: Compressing objects:  25% (3/12)\u001b[K\rremote: Compressing objects:  33% (4/12)\u001b[K\rremote: Compressing objects:  41% (5/12)\u001b[K\rremote: Compressing objects:  50% (6/12)\u001b[K\rremote: Compressing objects:  58% (7/12)\u001b[K\rremote: Compressing objects:  66% (8/12)\u001b[K\rremote: Compressing objects:  75% (9/12)\u001b[K\rremote: Compressing objects:  83% (10/12)\u001b[K\rremote: Compressing objects:  91% (11/12)\u001b[K\rremote: Compressing objects: 100% (12/12)\u001b[K\rremote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 12 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:   8% (1/12)   \rUnpacking objects:  16% (2/12)   \rUnpacking objects:  25% (3/12)   \rUnpacking objects:  33% (4/12)   \rUnpacking objects:  41% (5/12)   \rUnpacking objects:  50% (6/12)   \rUnpacking objects:  58% (7/12)   \rUnpacking objects:  66% (8/12)   \rUnpacking objects:  75% (9/12)   \rUnpacking objects:  83% (10/12)   \rUnpacking objects:  91% (11/12)   \rUnpacking objects: 100% (12/12)   \rUnpacking objects: 100% (12/12), done.\n",
            "From https://github.com/rkruser/ai4all-umd-2020\n",
            " * branch            master     -> FETCH_HEAD\n",
            "   3b22024..ab26f8e  master     -> origin/master\n",
            "Updating 3b22024..ab26f8e\n",
            "Fast-forward\n",
            " app_functions.py | 10 \u001b[32m+++++\u001b[m\u001b[31m---\u001b[m\n",
            " emily.py         | 73 \u001b[32m++++++++++++++++++++++++++\u001b[m\u001b[31m------------------------------\u001b[m\n",
            " portia.py        | 51 \u001b[32m+++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 3 files changed, 92 insertions(+), 42 deletions(-)\n",
            " create mode 100644 portia.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plRb0xUTeeW5",
        "colab_type": "text"
      },
      "source": [
        "# Exploring your trained networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z27boqxAg8Ck",
        "colab_type": "text"
      },
      "source": [
        "Let's load both the network that you designed and the standard resnet18 model. Both \"your_model_49\" and \"resnet_model_49\" have been trained on a small version of the Leafsnap dataset for 50 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CoYyeobjxBh",
        "colab_type": "text"
      },
      "source": [
        "Import libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSzdlU14jVPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import project_network as pnet\n",
        "import project_train as ptrain\n",
        "import data_loader\n",
        "import numpy as np\n",
        "from importlib import reload"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgFKjS5UjzDe",
        "colab_type": "text"
      },
      "source": [
        "Reload libraries only if necessary (if you've made changes after previously loading). Otherwise you can skip the next code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqI76lQajZ-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pnet = reload(pnet)\n",
        "ptrain = reload(ptrain)\n",
        "data_loader = reload(data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leOPNNcMj-J3",
        "colab_type": "text"
      },
      "source": [
        "Load the networks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bgHIXDCeh9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09a15cd7-50f5-42ee-951a-7c556680f157"
      },
      "source": [
        "device = 'cpu' #We don't need to bother with the GPU when testing the trained model on small sets of images\n",
        "\n",
        "your_net = pnet.YourNetwork(return_intermediates=True) # A special argument I added to your network --Ryen\n",
        "your_net.eval() # Put in testing mode\n",
        "net_weights, _ = torch.load('./models/your_model/your_model_49.pth',map_location=device) # The second return value is the optimizer weights, which we don't need now\n",
        "your_net.load_state_dict(net_weights)\n",
        "\n",
        "resnet18 = pnet.models.resnet18(pretrained=False, num_classes=185)\n",
        "resnet18.eval() # Put in testing mode\n",
        "resnet_weights, _ = torch.load('./models/resnet/resnet_model_49.pth',map_location=device) # The second return value is the optimizer weights, which we don't need now\n",
        "resnet18.load_state_dict(resnet_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ_xorNfhx94",
        "colab_type": "text"
      },
      "source": [
        "Now let's visualize a subset of leafsnap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4dA2BFOpK42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "reduce_size = transforms.Resize((600,600))\n",
        "to_tensor = transforms.ToTensor()\n",
        "to_pil = transforms.ToPILImage()\n",
        "leaf_species_name_mapping = data_loader.ClassLoader()\n",
        "\n",
        "# A function to conveniently print a list as a grid\n",
        "def print_list_grid(lst, nrow=8):\n",
        "  to_print = '[\\n['\n",
        "  for count, l in enumerate(lst):\n",
        "    to_print += str(l)+', '\n",
        "    if (count+1)%nrow == 0:\n",
        "      if count+1 < len(lst):\n",
        "        to_print += ']\\n['\n",
        "      else:\n",
        "        to_print += ']\\n'\n",
        "  to_print += ']'\n",
        "  print(to_print)\n",
        "\n",
        "def rescale(im_tensor, perc1 = 20, perc2 = 98):\n",
        "  numpy_tensor = im_tensor.numpy()\n",
        "  pc1 = np.percentile(numpy_tensor, perc1)\n",
        "  pc2 = np.percentile(numpy_tensor, perc2)\n",
        "  return ((im_tensor-pc1)/(pc2-pc1)).clamp(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQkvZbzl39I4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LeafSnapLoader = ptrain.LeafSnapLoader # Get objects like this so they can be easily reloaded if we change it\n",
        "default_im_transform = ptrain.default_im_transform\n",
        "\n",
        "\n",
        "leafsnap_test_dataset = LeafSnapLoader(mode='test',transform=default_im_transform)\n",
        "image_sample = []\n",
        "image_species_index = []\n",
        "image_species = []\n",
        "\n",
        "increment = len(leafsnap_test_dataset)//64\n",
        "\n",
        "# Get 64 leafsnap data samples spaced throughout the dataset\n",
        "for i in range(0, 64*increment, increment):\n",
        "  sample = leafsnap_test_dataset[i]\n",
        "  image_sample.append(sample['image'])\n",
        "  image_species_index.append(sample['species_index'])\n",
        "  image_species.append(sample['species'])\n",
        "\n",
        "im_tensor = torch.stack(image_sample)\n",
        "species_label_tensor = torch.LongTensor(image_species_index)\n",
        "im_grid = make_grid(im_tensor, nrow=8)\n",
        "im_grid = reduce_size(to_pil(im_grid))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn6z7_BEscDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(im_grid)\n",
        "print_list_grid(image_species)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJFmAHiBs2Q7",
        "colab_type": "text"
      },
      "source": [
        "Now let's run the neural networks on this sample of images and see how well they did."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvZ97wL9wZdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "  your_result, inter1, inter2 = your_net(im_tensor)\n",
        "  _, your_result = torch.max(your_result, 1)\n",
        "\n",
        "  resnet_result = resnet18(im_tensor)\n",
        "  _, resnet_result = torch.max(resnet_result, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBTSIAzErBNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "your_num_correct = (your_result == species_label_tensor).sum()\n",
        "your_species_predictions = [leaf_species_name_mapping.ind2str(ind.item()) for ind in your_result]\n",
        "your_correctness_grid = (your_result == species_label_tensor).view(8,8)\n",
        "\n",
        "resnet_num_correct = (resnet_result == species_label_tensor).sum()\n",
        "resnet_species_predictions = [leaf_species_name_mapping.ind2str(ind.item()) for ind in resnet_result]\n",
        "resnet_correctness_grid = (resnet_result == species_label_tensor).view(8,8)\n",
        "\n",
        "print(\"Your model got {0} out of {1}\".format(your_num_correct,64))\n",
        "print_list_grid(your_species_predictions)\n",
        "print(your_correctness_grid)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"resnet18 got {0} out of {1}\".format(resnet_num_correct,64))\n",
        "print_list_grid(resnet_species_predictions)\n",
        "print(resnet_correctness_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CGk3BohBtEH",
        "colab_type": "text"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUH9xcC5BvQ9",
        "colab_type": "text"
      },
      "source": [
        "Find images in the above grid that:\n",
        "\n",
        "* Both models classified correctly\n",
        "* Your model got right but resnet got wrong\n",
        "* Resnet got right but your model got wrong\n",
        "* Both your models got wrong\n",
        "\n",
        "For the wrong images, find an image in LeafSnap of the species that the model(s) mistakenly thought were the true classification. (You can find images of various species in data/leafsnap/dataset/images/field/{species_name}/). Compare these wrong images with images of the true species.\n",
        "\n",
        "Include these examples in your presentation and see if you can explain why the network may have gotten the classification wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb9Vy7j-0P6b",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing convolutional activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk_N81wL0406",
        "colab_type": "text"
      },
      "source": [
        "Let's visualize the convolutional activations of your network on one or two of the leafsnap test images.\n",
        "\n",
        "Above, when we run your network, we extract \"inter1\" and \"inter2\". If you look inside project_network.py, you will see that these correspond to the outputs of your first convolution-relu-pooling and your second convolution-relu-pooling layers respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRQelBNG1LQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(inter1.size())\n",
        "print(inter2.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K8Mc8fM1v-m",
        "colab_type": "text"
      },
      "source": [
        "As you can see from the sizes printed above, each intermediate layer is a batch of 64 convolutional image activations. Each \"image\" in inter1 has 64 convolutional channels, and each \"image\" in inter2 has 128 convolutional channels. Each channel corresponds to a different convolution operation learned by the neural network.\n",
        "\n",
        "Let's visualize the convolutions of the third image in this batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ9kwU986SdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standard_size = transforms.Resize((600,600),interpolation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE1jh5fg2Y8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acer_palmatum_inter1 = inter1[2].unsqueeze(1)\n",
        "acer_palmatum_inter2 = inter2[2,:64].unsqueeze(1) #Take first 64 conv channels\n",
        "\n",
        "# Unsqueeze dimension 1 to get 64 x 1 x 55 x 55 and 64 x 1 x 13 x 13\n",
        "# This allows us to pretend that the convolution channels are like a batch of 64 black and white images\n",
        "\n",
        "acer_palmatum_inter1_conv_grid = make_grid(acer_palmatum_inter1, nrow=8, normalize=True,\n",
        "                                    scale_each = True)\n",
        "acer_palmatum_inter2_conv_grid = make_grid(acer_palmatum_inter2, nrow=8, normalize=True,\n",
        "                                    scale_each = True)\n",
        "\n",
        "# Use the rescaling function to make the outputs look prettier\n",
        "acer_palmatum_inter1_conv_grid = rescale(acer_palmatum_inter1_conv_grid)\n",
        "acer_palmatum_inter2_conv_grid = rescale(acer_palmatum_inter2_conv_grid)\n",
        "\n",
        "print(\"Acer_palmatum first set of conv/pooling layers\")\n",
        "display(standard_size(to_pil(acer_palmatum_inter1_conv_grid)))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(\"Acer_palmatum second set of conv/pooling layers\")\n",
        "display(standard_size(to_pil(acer_palmatum_inter2_conv_grid)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDKrdSjl6jlE",
        "colab_type": "text"
      },
      "source": [
        "Note the increasing level of shape abstraction in the convolution outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la_UBA0d6uTj",
        "colab_type": "text"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dsf3WjN6wig",
        "colab_type": "text"
      },
      "source": [
        "Pick a different leaf image and visualize its intermediate convolution layers in the same way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqBcBf9R6v0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dV7bqD605gK",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTNqI_RyRoW",
        "colab_type": "text"
      },
      "source": [
        "Now let's run the networks on the entire test dataset and compute the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "203wvt61v6ZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Testing your network\")\n",
        "your_net.return_intermediates = False\n",
        "ptrain.test(your_net)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(\"Testing resnet18\")\n",
        "ptrain.test(resnet18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdIY7cO7qPt",
        "colab_type": "text"
      },
      "source": [
        "## Training results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXVdbXfl7xsp",
        "colab_type": "text"
      },
      "source": [
        "The training results are saved in models/resnet/stats_49.pkl and models/your_model/stats_49.pkl. There are also the \"print_output.txt\" files in the same directory, which show all output printed to the screen during training (you can open this file in Colab to see what training looked like).\n",
        "\n",
        "The stats_49.pkl files are python *pickle* files. That is, they are python objects saved using the pickle library. Let's load these objects for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxsFv_S-y0KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "your_model_train_losses, your_model_val_accuracies = pickle.load(open('./models/your_model/stats_49.pkl','rb'))\n",
        "resnet_model_train_losses, resnet_model_val_accuracies = pickle.load(open('./models/resnet/stats_49.pkl','rb'))\n",
        "\n",
        "print(your_model_train_losses)\n",
        "print(your_model_val_accuracies)\n",
        "print(resnet_model_train_losses)\n",
        "print(resnet_model_val_accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fBko3JB9RQq",
        "colab_type": "text"
      },
      "source": [
        "Train_losses and val_accuracies are both lists of 50 entries, one for each training epoch. Train_losses holds the average loss for each epoch. Val_accuracies holds the percentage accuracy of the model on the validation set after each epoch. \n",
        "\n",
        "**Note** that val_accuracies is a list of tuples (epoch_number, accuracy), so you'll have to write a small bit of code to extract the accuracies into their own list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLtVCBSP-R7l",
        "colab_type": "text"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITHcOweA-TmD",
        "colab_type": "text"
      },
      "source": [
        "Plot the training losses and validation accuracies versus num_epochs for each network. Make one plot for losses and one plot for accuracies, and on each plot have two lines, one for your model and one for resnet.\n",
        "\n",
        "Example of how to plot two lines on a graph:\n",
        "\n",
        "```\n",
        "plt.xlabel(\"X-axis name\")\n",
        "plt.ylabel(\"Y-axis name\")\n",
        "plt.title(\"Title of figure\")\n",
        "plt.plot( values, label=\"Line label\" )\n",
        "plt.plot( values2, label=\"Line 2 label\" )\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI1zZMNt8uJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Your plotting code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhII5teRC206",
        "colab_type": "text"
      },
      "source": [
        "### Exercise: Analyzing the training curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qURtT1aKC7fr",
        "colab_type": "text"
      },
      "source": [
        "1. After how many epochs does performance mostly stop improving for each model?\n",
        "2. Which model learns faster?\n",
        "3. Which model achieves higher validation accuracy?\n",
        "4. Can you see any evidence of overfitting in the training curves?\n",
        "5. Compare the validation accuracies with the test set accuracies from earlier in this colab. According to the test data, which model is better?\n",
        "6. Why use a separate validation and test set?\n",
        "7. There are 185 tree species in this dataset. If some model guesses the true classification at random, what would you expect the model accuracy to be on the test data? How well does your model do compared to a completely random model?\n",
        "\n",
        "Note that resnet is almost certainly going to do better than your model. This is because the design of resnet is highly optimized by professional machine learning researchers, so don't feel too bad about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBxq2bIkBYfQ",
        "colab_type": "text"
      },
      "source": [
        "# Putting it all together in the Web app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO1idXkNKrf7",
        "colab_type": "text"
      },
      "source": [
        "Here we have our web app code. The app will take a user-uploaded image and:\n",
        "\n",
        "* Run the pretrained imagenet resnet50 to predict what the object is\n",
        "* Run your leafsnap model on the image in case the image is a leaf\n",
        "* Run each of your individual image transforms on the input and display the results on the web page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqqnnLT7Ggta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run resnet50 pretrained on imagenet, your neural network model,\n",
        "#  and all student image transformations\n",
        "#  Collect results in a dictionary\n",
        "import traceback\n",
        "from flask import Flask, jsonify, request, render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "#from utils import read_file, transform_image, get_topk, model  #render_prediction\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "import project_network as pnet\n",
        "import project_train as ptrain\n",
        "import data_loader\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import base64\n",
        "\n",
        "from data_loader import ClassLoader\n",
        "\n",
        "# Student image transforms\n",
        "# from Anu import ...\n",
        "# from shubham import ...\n",
        "# from emily import ...\n",
        "# from portia import ...\n",
        "# from kemka import ...\n",
        "# from unity import ...\n",
        "\n",
        "\n",
        "def read_file(upload=None, url=None):\n",
        "    if (upload is not None) and upload.filename:\n",
        "        in_memory_file = BytesIO()\n",
        "        upload.save(in_memory_file)\n",
        "        img = Image.open(in_memory_file)\n",
        "        return img\n",
        "\n",
        "    elif url is not None:\n",
        "        response = requests.get(url)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        return img\n",
        "\n",
        "    else:\n",
        "        raise NameError('Invalid file/url')\n",
        "\n",
        "def to_base64(img):\n",
        "    buffered = BytesIO()\n",
        "    img.save(buffered, format=\"JPEG\")\n",
        "    return base64.b64encode(buffered.getvalue()).decode('ascii')\n",
        "\n",
        "# Transform input into the form our model expects\n",
        "def transform_image(pil_image):\n",
        "    input_transforms = [\n",
        "        transforms.Resize(255),           # We use multiple TorchVision transforms to ready the image\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.485, 0.456, 0.406],       # Standard normalization for ImageNet model input\n",
        "            [0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ]\n",
        "    my_transforms = transforms.Compose(input_transforms)\n",
        "    timg = my_transforms(pil_image)                       # Transform PIL image to appropriately-shaped PyTorch tensor\n",
        "    timg.unsqueeze_(0)                                    # PyTorch models expect batched input; create a batch of 1\n",
        "    return timg\n",
        "\n",
        "leafsnap_transform_image = transforms.Compose([\n",
        "  transforms.Resize((224,224)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def get_topk(model, input_tensor, k=5):\n",
        "    outputs = model(input_tensor)                 # Get likelihoods for all ImageNet classes\n",
        "    values, indices = torch.topk(outputs, k)              # Extract top k most likely classes\n",
        "    values = values.data.cpu().numpy()[0]\n",
        "    indices = indices.data.cpu().numpy()[0]\n",
        "    return values, indices\n",
        "\n",
        "resnet50_imagenet_model = models.resnet50(pretrained=True)\n",
        "resnet50_imagenet_model.eval()\n",
        "img_class_map = None\n",
        "mapping_file_path = 'index_to_name.json'                  # Human-readable names for Imagenet classes\n",
        "if os.path.isfile(mapping_file_path):\n",
        "    with open (mapping_file_path) as f:\n",
        "        img_class_map = json.load(f)\n",
        "\n",
        "device = 'cpu' #We don't need to bother with the GPU when testing the trained model on small sets of images\n",
        "your_net = pnet.YourNetwork() # A special argument I added to your network --Ryen\n",
        "net_weights, _ = torch.load('./models/your_model/your_model_49.pth',map_location=device) # The second return value is the optimizer weights, which we don't need now\n",
        "your_net.load_state_dict(net_weights)\n",
        "your_net.eval()\n",
        "leaf_species_name_mapping = ClassLoader()\n",
        "downsize = transforms.Resize(60)\n",
        "\n",
        "# Need to return a list of dictionaries with keys 'model', 'label', 'score', 'image'\n",
        "# Ryen will write this function and get back to you\n",
        "def collect_outputs(input_pil_image):\n",
        "  resnet50_im = transform_image(input_pil_image)\n",
        "  your_net_im = leafsnap_transform_image(input_pil_image).unsqueeze(0)\n",
        "\n",
        "  r50_vals, r50_inds = get_topk(resnet50_imagenet_model, resnet50_im, 5)\n",
        "  your_vals, your_inds = get_topk(your_net, your_net_im, 5)\n",
        "\n",
        "  image_net_results = []\n",
        "  for value, idx in zip(r50_vals, r50_inds):\n",
        "    image_net_results.append({\n",
        "        \"model\": \"ImageNet Resnet50 Pretrained\",\n",
        "        \"category\": img_class_map.get(str(idx), \"Unknown\")[1],\n",
        "        \"score\": str(value),\n",
        "        \"image\": None\n",
        "    })\n",
        "\n",
        "  your_net_results = []\n",
        "  for value, idx in zip(your_vals, your_inds):\n",
        "    species_name = leaf_species_name_mapping.ind2str(idx)\n",
        "    species_dir = os.path.join('./data/leafsnap/dataset/images/field/',species_name)\n",
        "    species_file = os.listdir(species_dir)[0]\n",
        "    species_file = os.path.join(species_dir, species_file)\n",
        "    your_net_results.append({\n",
        "        \"model\": \"Your Network Trained on Leafsnap\",\n",
        "        \"category\": species_name,\n",
        "        \"score\": str(value),\n",
        "        \"image\": to_base64(downsize(Image.open(species_file)))\n",
        "    })\n",
        "\n",
        "  # In place of \"None\", write\n",
        "  #  to_base64( student_transform( img ) )\n",
        "  #  student_transform must take a PIL image and return a PIL image\n",
        "  #  The returned image must be no larger than 256 by 256, preferably smaller\n",
        "  student_image_transforms = [\n",
        "    {\n",
        "      \"model\": \"Shubham\",\n",
        "      \"category\": '-',\n",
        "      \"score\": '-',\n",
        "      \"image\": None \n",
        "    },\n",
        "    {\n",
        "      \"model\": \"Portia\",\n",
        "      \"category\": '-',\n",
        "      \"score\": '-',\n",
        "      \"image\": None # Fill this in    \n",
        "    },\n",
        "    {\n",
        "      \"model\": \"Kemka\",\n",
        "      \"category\": '-',\n",
        "      \"score\": '-',\n",
        "      \"image\": None # Fill this in    \n",
        "    },\n",
        "    {\n",
        "      \"model\": \"Emily\",\n",
        "      \"category\": '-',\n",
        "      \"score\": '-',\n",
        "      \"image\": None # Fill this in    \n",
        "    },\n",
        "    {\n",
        "      \"model\": \"Unity\",\n",
        "      \"category\": '-',\n",
        "      \"score\": '-',\n",
        "      \"image\": None # Fill this in    \n",
        "    },\n",
        "    {\n",
        "      \"model\": \"Anu\",\n",
        "      \"category\": '-',\n",
        "      \"score\": '-',\n",
        "      \"image\": None # Fill this in    \n",
        "    },\n",
        "  ]\n",
        "\n",
        "  all_results = image_net_results + your_net_results + student_image_transforms\n",
        "\n",
        "  return all_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0LHAnLmGRce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route('/', methods=['GET'])\n",
        "def root():\n",
        "    return render_template('index.html')\n",
        "\n",
        "# new test comment\n",
        "\n",
        "@app.route('/predict', methods=['GET', 'POST'])\n",
        "def predict():\n",
        "    if request.method == 'GET':\n",
        "        try:\n",
        "            url = request.args.get('q')\n",
        "            app.logger.debug('url provided - %s', url)\n",
        "            input_tensor = transform_image(read_file(url=url))\n",
        "            values, indices = get_topk(input_tensor)\n",
        "            results = render_prediction(values, indices)\n",
        "            return jsonify(results=results)\n",
        "\n",
        "        except:\n",
        "            app.logger.debug(\"Error: %s\", traceback.print_exc())\n",
        "            return jsonify(\"invalid image url\")\n",
        "\n",
        "    elif request.method == 'POST':\n",
        "        try:\n",
        "            file = request.files['file']\n",
        "            app.logger.debug('file uploaded - %s', file)\n",
        "            url = request.form.get(\"url\", None)\n",
        "            app.logger.debug('url provided - %s', url)\n",
        "\n",
        "            input_pil_image = read_file(upload=file, url=url)\n",
        "            #values, indices = get_topk(input_tensor)\n",
        "            results = collect_outputs(input_pil_image)\n",
        "            return jsonify(results=results)\n",
        "\n",
        "        except:\n",
        "            app.logger.debug(\"Error: %s\", traceback.print_exc())\n",
        "            return jsonify(\"invalid image\")\n",
        "\n",
        "    else:\n",
        "        app.logger.debug(\"Error: %s\", traceback.print_exc())\n",
        "        return jsonify('invalid request')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-gHkCx9K92j",
        "colab_type": "text"
      },
      "source": [
        "When ready to run the app, execute the following code and use the printed ngrok.io link to go to the web page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq3CyopAGUBy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "85556d77-2033-430d-8143-63f6ceb0f9c5"
      },
      "source": [
        "run_with_ngrok(app)\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://efcf2be3e571.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/Jul/2020 19:02:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Jul/2020 19:02:00] \"\u001b[37mGET /static/main.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Jul/2020 19:02:00] \"\u001b[37mGET /static/main.js HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/Jul/2020 19:02:01] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [30/Jul/2020 19:02:45] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWtYKmLpvU2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}